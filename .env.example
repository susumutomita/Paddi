# AI Provider Configuration
# Choose between 'gemini' or 'ollama'
AI_PROVIDER=gemini

# Gemini/Vertex AI Configuration
GCP_PROJECT_ID=example-project-123
VERTEX_AI_LOCATION=asia-northeast1
VERTEX_AI_MODEL=gemini-1.5-flash

# Ollama Configuration
# Model options: llama3, codellama, mistral, etc.
OLLAMA_MODEL=llama3
OLLAMA_ENDPOINT=http://localhost:11434

# AI Generation Settings
AI_TEMPERATURE=0.2
AI_MAX_OUTPUT_TOKENS=2048

# General Settings
USE_MOCK=true
DATA_DIR=data
OUTPUT_DIR=output